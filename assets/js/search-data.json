{
  
    
        "post0": {
            "title": "Intro to Stereo Vision",
            "content": "Intro to Stereo Vision . Stereo vision . The idea is to take a particular object and observe it from 2 slightly different viewpoints - in this way we get an understanding of shape from the idea of motion. Each eye (or camera) sees slightly different image. . . Anaglyph stereo image - image resulted using encoding each eye’s image using filters of different colors, typically red and cyan. . E.g. if you put some blue filter on the image it will be blue. If you put some red filter on top - the resulted image will be dark. . Basic idea . Two images from two cameras taken under slightly different viewpoints (gif below). | Notable fact - parts in front go in particular way (on the left) and the parts in the | From 2 different viewpoints we get a sense of how the parts move (back or forth). | . via Gfycat . Geometry of stereo . Cameras are defined by their optical centers. | 2 cameras are looking at some scene point. | If we can figure out what are 2 points in 2 cameras are the same scene point. | Furthermore, if I can figure out which way the cameras are pointed, we can figure out the depth of that point (arrow in the image). In order to estimate the depth (the shape between 2 views) there are 2 things we have to consider: . | The pose of cameras (so-called the camera “calibration”) | Image points correspondences (which point corresponds to which) - for example, this red dot on 2 images. | . We are going to talk about the image points correspondence first. . Geometry for a simple stereo system . First, we assume the parallel optical axes, known camera parameters - or, calibrated cameras. | The image planes are coplanar - they are in the same plane. The schema below as we are looking down on cameras system. | We assume that our cameras are separated by baseline B and both cameras have a focal length f. | The point P is located at the distance Z in camera coordinate system. Thus, Z is a distance from point P all the way down to the center of projection. | Now, we can show how the point P projectes into both the left and right images. | X_l (positive) - the distance to the left optic axis. X_r (left) - distance to the right optic axis. | . Depth from disparity . . Since depth is distance to the object and disparity is the inverse proportional to depth, the brightest values on disparity map D(x,y) are closest to camera. | Disparity in a simple words - difference of X coordinates between point in left and right images. | .",
            "url": "https://machinelearninghelsinki.github.io/mlstudygroup/learning/2021/02/13/Stereo-Vision.html",
            "relUrl": "/learning/2021/02/13/Stereo-Vision.html",
            "date": " • Feb 13, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Meetup Sept 19 2020 (Notes)",
            "content": "Notes . New ML pages introduction and resources sharing. | Aditya shared the Multilingual Natural Processing course, going through the lecture slides and assignment. | Artyom finished and presented Image processing with OpenCV: Part 1 | . Meetup Videos . TODO | . Links shared . Link Description . Advanced ML course | EDX link for Advanced ML course | . [OSCAR Open Super-large Crawled ALMAnaCH coRpus](https://oscar-corpus.com/ | Multilingual corpus obtained by language classification and filtering | . Some relevant Youtube resources . Footnotes .",
            "url": "https://machinelearninghelsinki.github.io/mlstudygroup/meetup/2020/09/19/MLMeetupNotes.html",
            "relUrl": "/meetup/2020/09/19/MLMeetupNotes.html",
            "date": " • Sep 19, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Mlmeetupnotes",
            "content": "&lt;!DOCTYPE html&gt;&lt;html lang=&quot;zh-cmn-Hans&quot;&gt;&lt;head&gt; . &lt;/head&gt; . . toc: true layout: post description: Notes, links and thoughs from the meetup on Sept 19th 2020 categories: [meetup] comments: true . title: Meetup Sept 19 2020 (Notes) . Notes . New ML pages introduction and resources sharing. | Aditya shared the Multilingual Natural Processing course, going through the lecture slides and assignment. | Artyom finished and presented Image processing with OpenCV: Part 1 | . Meetup Videos . TODO | . Links shared . Link Description . Advanced ML course | EDX link for Advanced ML course | . [OSCAR Open Super-large Crawled ALMAnaCH coRpus](https://oscar-corpus.com/ | Multilingual corpus obtained by language classification and filtering | . Some relevant Youtube resources . Footnotes . [^1]: Will add more things.. . &lt;/html&gt; .",
            "url": "https://machinelearninghelsinki.github.io/mlstudygroup/2020/09/19/MLMeetupNotes.html",
            "relUrl": "/2020/09/19/MLMeetupNotes.html",
            "date": " • Sep 19, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "RL and Robotics Learning Resources",
            "content": "Robotics . Mobile Sensing and Robotics . Sensors . Advanced Robotics . Reinforcement Learning . Introduction to Reinforcement Learning . Very good course by David Silver. Highly recommend it! . Reinforcement Learning (University of Waterloo) . Deep RL course Emma Brunskill . Easy to follow lectures and also has a course page where you can solve the assignments . Deep RL course Pieter Abbeel . Highly recommend these lectures as they go indepth into the mathematics . Meta Learning . Good course on metalearning and has vast applications in RL and Robotics .",
            "url": "https://machinelearninghelsinki.github.io/mlstudygroup/resources/2020/09/17/ReinforcementLearning.html",
            "relUrl": "/resources/2020/09/17/ReinforcementLearning.html",
            "date": " • Sep 17, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Meetup Sept 12 2020 (Notes)",
            "content": "Notes . We introduced the new Machine Learning Meetup page and blogs related to it. | Aditya discussed the first part of Reinforcement Learning section from the course | Artyom went through Image processing with OpenCV: Part 1 | . Meetup Videos . TODO | . Links shared . Link Description . Advanced ML course | EDX link for Advanced ML course | . Some relevant Youtube resources . Footnotes .",
            "url": "https://machinelearninghelsinki.github.io/mlstudygroup/meetup/2020/09/12/MLMeetupNotes.html",
            "relUrl": "/meetup/2020/09/12/MLMeetupNotes.html",
            "date": " • Sep 12, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Image processing with OpenCV: Part 1",
            "content": "Image processing with OpenCV: Part 1 . Disclaimer . The notebook is prepared on initial repo: https://github.com/MakarovArtyom/OpenCV-Python-practices . Acknowledgement . The code from Jupyter notebooks follows Udacity Computer Vision nanodegree logic. More Computer Vision exercises can be found under Udacity CVND repo: https://github.com/udacity/CVND_Exercises . Libraries . # uncomment in case PyQt5 is not installed #! pip install PyQt5 . import numpy as np import matplotlib.pyplot as plt # for image reading import matplotlib.image as mpimg import cv2 # makes image pop-up in interactive window %matplotlib qt . Image reading . Given an RGB image, let’s read it using matplotlib (mpimg) . . The RGB color model is an additive color model in which red, green, and blue light are added together in various ways to reproduce a broad array of colors. The name of the model comes from the initials of the three additive primary colors, red, green, and blue . Source: https://en.wikipedia.org/wiki/RGB_color_model . image = mpimg.imread(&#39;images/robot_blue.jpg&#39;) . print(&#39;Shape of the image:&#39;, image.shape) . Shape of the image: (720, 1280, 3) . Per single image, we can retrieve the following info: . height = 720 | width = 1280 | number of channels = 3 | . Later on, we will see that for many “classical” computer vision algorithms the color information is redundant. For solving problems like edge, corner or blob detection, the grayscale image is enough. . Let’s use cvtColor function to convert RGB image to gray. . gray_img = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) plt.imshow(gray_img, cmap = &#39;gray&#39;) . &lt;matplotlib.image.AxesImage at 0x7fdd7c819828&gt; . . print(&#39;Shape of the image after convertation:&#39;, gray_img.shape) . Shape of the image after convertation: (720, 1280) . It’s important to understand an image is just a matrix (tensor) with values, representing the intensity of pixels (from 0 to 255). Each pixel has it’s own coordinates. . print(&#39;Max pixel value:&#39;, gray_img.max()) print(&#39;Min pixel value:&#39;, gray_img.min()) . Max pixel value: 255 Min pixel value: 0 . Use the pair of (x,y) coordinates to access particular pixel’s value. . x = 45 y = 52 pixel_value = gray_img[x,y] print(pixel_value) . 28 . Note, it’s possible to manipulate pixel values by scalar multiplication and augmentation in different formats. . Example: let’s create a 6x6 image with random pixel values. . pixel = abs(np.random.randn(6,6)) * 10 pixel.astype(int) . array([[15, 8, 3, 14, 3, 3], [ 6, 9, 10, 7, 0, 5], [11, 18, 10, 4, 14, 5], [ 9, 6, 0, 1, 1, 16], [18, 9, 6, 29, 8, 7], [12, 0, 2, 0, 9, 12]]) . Next, display the results with grayscale colormap. . plt.matshow(pixel, cmap = &#39;gray&#39;) . &lt;matplotlib.image.AxesImage at 0x7fdd7acd96d8&gt; . . Color isolating . %matplotlib inline . Let’s read an image and see how we can visualize 3 RBG colors individually. . img = mpimg.imread(&#39;images/car.jpeg&#39;) plt.imshow(img) . &lt;matplotlib.image.AxesImage at 0x7feaec8de0b8&gt; . . print(&#39;Shape of the image:&#39;, img.shape) . Shape of the image: (281, 500, 3) . Since the color image is a tensor composed of 3 colors, we can express “red”, “green” and “bleu” as follows. . r = img[:,:, 0] # red g = img[:,:, 1] # green b = img[:,:, 2] # blue print(&#39;Shape of individual color matrix:&#39;, r.shape) . Shape of individual color matrix: (281, 500) . f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20,10)) # visualize red ax1.set_title(&#39;R channel&#39;) ax1.imshow(r, cmap = &#39;gray&#39;) # green ax2.set_title(&#39;G channel&#39;) ax2.imshow(g, cmap = &#39;gray&#39;) # blue ax3.set_title(&#39;B channel&#39;) ax3.imshow(b, cmap = &#39;gray&#39;) . &lt;matplotlib.image.AxesImage at 0x7feb0295d550&gt; . . Background change . Isolating object’s background . Additional resources: . Color picker to choose boundaries: https://www.w3schools.com/colors/colors_picker.asp | . Suppose we have an image of an object on solid color background. Consider the image of a robot below - we read it with open-cv library. . img = cv2.imread(&#39;images/robot_blue.jpg&#39;) plt.imshow(img) . &lt;matplotlib.image.AxesImage at 0x7feaec56f6d8&gt; . . Important note: open-cv reads images as BGR, not RGB - as a result the output is different from original image. In particular - red and blue colors are in reverse order. . Next, we take a copy of image and change it from BGR to RGB (pass parameter cv2.COLOR_BGR2RGB). Note, that any transformations applied to a copy will not effect an image. . &quot;&quot;&quot; - make a copy with numpy - change from BGR to RGB &quot;&quot;&quot; img_copy = np.copy(img) img_copy = cv2.cvtColor(img_copy, cv2.COLOR_BGR2RGB) plt.imshow(img_copy) . &lt;matplotlib.image.AxesImage at 0x7feaec6dc4e0&gt; . . First, define color a threshold. It will be a lower and upper bounds for the background I wnat to isolate. Here we need to specify 3 values - for each color - red, green and blue. . Lower bound: red and green as zero, and high value for blue. For example, 230. | Upper bound: red, green - some small values and blue - maximum, i.e. 250. So, allow a bit red and green. | . All values within this range will be considered as intense blue color. This range will be replaced by another background. . blue_lower = np.array([0, 0, 230]) blue_upper = np.array([50, 50, 250]) . Creating mask . We are going to use a common way to isolate chosen area - creating a mask. Now, we will isolate blue background area using inRange() function. It verifies each pixel whether it falls into specific range, defined by lower and upper bounds. . If it falls - the white mask will be displayed; | If it does not fall within the range - pixel will be turned into black. | . Simply saying, everything inside the interval will be white. . &quot;&quot;&quot; - function will take lower and upper bounds of image - and define the mask &quot;&quot;&quot; mask = cv2.inRange(img_copy, blue_lower, blue_upper) plt.imshow(mask, cmap = &#39;gray&#39;) . &lt;matplotlib.image.AxesImage at 0x7feaec74afd0&gt; . . White area - area, where the new image background will be displayed. | Black area - where ew image background will be blocked out. | . The mask has the same height and width, and each pixel has 2 values: . 255 - for white area; | 0 - for black area. | . Now, let’s put our object into a mask. . First of all, create a copy of image (just if we want to change it later on). | Then, one way to separate blue background from this is to check where blue pixels intersect with mask white pixels, or in other words - do not intersect with black (not equal 0). We will set them to black color. | . Displaying an image, we will see that only object appears on a black background. . masked_img = np.copy(img_copy) masked_img[mask!=0] = [0,0,0] plt.imshow(masked_img) . &lt;matplotlib.image.AxesImage at 0x7feaec47eba8&gt; . . Applying new background . Next step will be just to apply new background on top of black one. We just take an image, (e.g. “space image” below and convert it from BGR to RGB). . background_img = cv2.imread(&#39;images/background_img.jpg&#39;) background_img = cv2.cvtColor(background_img, cv2.COLOR_BGR2RGB) plt.imshow(background_img) . &lt;matplotlib.image.AxesImage at 0x7feaec4e4630&gt; . . It’s also important to crop background image to make it the same size as our robot image. We crop it by height and width. . &quot;&quot;&quot; - cropping to a size of (720, 1280) &quot;&quot;&quot; crop_background_img = background_img[0:720, 0:1280] . Now, we will do an opposite operation with a cropped background image: choose pixels that are equal to 0 in mask image (there we had black robot) and set these pixels to a black on cropped background image. . Simply saying, we merge mask and crop_background_img. . crop_background_img[mask == 0] = [0,0,0] plt.imshow(crop_background_img) . &lt;matplotlib.image.AxesImage at 0x7feaeb3c8278&gt; . . Final step: add object and new background together . Since the black area on cropped background image is equivalent to 0, we can add this image to masked image. In this case simple summation will work, since we deal with matrices. . final_image = crop_background_img + masked_img plt.imshow(final_image) . &lt;matplotlib.image.AxesImage at 0x7feaeb425be0&gt; . . Object detection in HSV space . HSV colorspace . HSV colorspace is similar to RGB and also represents 3 channel - hue, saturation and value. This space is commonly used in tasks like image segmentation. In HSV colorspace Hue channel models color type, while Saturation and Value represents color as a mixture of shades and brightness. . Since the hue channel models the color type, it is very useful in image processing tasks that need to segment objects based on its color. Variation of the saturation goes from unsaturated to represent shades of gray and fully saturated (no white component). Value channel describes the brightness or the intensity of the color. . Source: https://docs.opencv.org/3.4/da/d97/tutorial_threshold_inRange.html . We will start with image displaying. We will use both RGB and HSV color spaces to detect the balloons in the image. . img = cv2.imread(&#39;images/baloons.jpg&#39;) img_copy = np.copy(img) img_copy = cv2.cvtColor(img_copy, cv2.COLOR_BGR2RGB) plt.imshow(img_copy) . &lt;matplotlib.image.AxesImage at 0x7feaef450a58&gt; . . First, let’s plot the color channels in RGB space. . # RGB channels # we take all pixels, but isolate the color we need red = img_copy[:,:,0] green = img_copy[:,:,1] blue = img_copy[:,:,2] # now, plot each of this colors in gray scale to see the relative intensities f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize = (20,10)) ax1.set_title(&#39;Red scale&#39;) ax1.imshow(red, cmap = &#39;gray&#39;) ax2.set_title(&#39;Green scale&#39;) ax2.imshow(green, cmap = &#39;gray&#39;) ax3.set_title(&#39;Blue scale&#39;) ax3.imshow(blue, cmap = &#39;gray&#39;) . &lt;matplotlib.image.AxesImage at 0x7feaefa82630&gt; . . We can see that pink balloons have high values for the red (close to 255, white color) and medium values for blue. However, there are many variations, especially if the balloon is in shadow. . Next, we convert image from RGB to HSV. . hsv = cv2.cvtColor(img_copy, cv2.COLOR_RGB2HSV) . Now, we isolate each of these channels as we did before. . hue = hsv[:,:,0] saturation = hsv[:,:,1] value = hsv[:,:,2] f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize = (20,10)) ax1.set_title(&#39;Hue&#39;) ax1.imshow(hue, cmap = &#39;gray&#39;) ax2.set_title(&#39;Saturation&#39;) ax2.imshow(saturation, cmap = &#39;gray&#39;) ax3.set_title(&#39;Value&#39;) ax3.imshow(value, cmap = &#39;gray&#39;) . &lt;matplotlib.image.AxesImage at 0x7feaf0b542b0&gt; . . Compare this to our image. Take a look at pink balloons: . Hue has high values for pink balloons, and even in shadow hue level is pretty consistent; | Saturation values are quite versatile depending on shadow. | . Create thresholds for pink color in RGB and HSV . Now, create lower and upper bounds for pink color in RGB and then in HSV spaces. . # use thrashold for pink color in RGB # link to check: https://www.w3schools.com/colors/colors_rgb.asp lower_pink = np.array([180,0,100]) upper_pink = np.array([255,255,230]) . Then, do the same for HSV. Remember, that hue goes from 0 to 180 degrees. We will limit this from 160 to 180. Will allow any values for saturation and value from 0 to 255. . lower_hue = np.array([140,0,0]) upper_hue = np.array([180,255,255]) . Mask the images . Now, we will create a mask to see how well these thresholds select the pink balloons. As previous we will use inRage() function and apply mask afterwards. . &quot;&quot;&quot; - first we create mask based on lower and upper bounds. Mask will assign 0 to all pixels that are out of these interval. - then we make all pixels, intersecting with black color (have 0 values) black (assign to [0,0,0]). - isolate pink color &quot;&quot;&quot; mask_rgb = cv2.inRange(img_copy, lower_pink, upper_pink) masked_image = np.copy(img_copy) masked_image[mask_rgb == 0] = [0,0,0] plt.imshow(masked_image) plt.title(&#39;RGB selection&#39;) . Text(0.5, 1.0, &#39;RGB selection&#39;) . . Here we can see RGB colorspace: . Does not select all pink balloons (or ones that are in shadow); | Selects some other colors. | . Finally, repeat the procedure for HSV colorspace. . &quot;&quot;&quot; - note, we will use hsv image (RGB2HSV) &quot;&quot;&quot; mask_hsv = cv2.inRange( hsv, lower_hue, upper_hue) masked_image = np.copy(hsv) masked_image[mask_hsv == 0] = [0,0,0] plt.imshow(masked_image) plt.title(&#39;HSV selection&#39;) . Text(0.5, 1.0, &#39;HSV selection&#39;) . . Day and Night Image Classifier . The day/night image dataset consists of 200 RGB color images in two categories: day and night. There are equal numbers of each example: 100 day images and 100 night images. . We’d like to build a classifier that can accurately label these images as day or night, and that relies on finding distinguishing features between the two types of images! . Note: All images come from the AMOS dataset (Archive of Many Outdoor Scenes). . Training and Testing Data . The 200 day/night images are separated into training and testing datasets. . 60% of these images are training images, for you to use as you create a classifier. | 40% are test images, which will be used to test the accuracy of your classifier. | . First, we set some variables to keep track of some where our images are stored: . `image_dir_training`: the directory where our training image data is stored `image_dir_test`: the directory where our test image data is stored . # Image data directories image_dir_training = &quot;training&quot; image_dir_test = &quot;test&quot; . Load the datasets . These first few lines of code will load the training day/night images and store all of them in a variable, IMAGE_LIST. This list contains the images and their associated label (“day” or “night”). . For example, the first image-label pair in IMAGE_LIST can be accessed by index: . python # Using the load_dataset function in helpers.py # Load training data IMAGE_LIST = helpers.load_dataset(image_dir_training) . Train dataset contains: . Images, classified as “day” or “night”; | It’s possible to access first pair “image-label” by index 0 - IMAGE_LIST[0] | To access label of e.g. first image - IMAGE_LIST[0][0]. | . Visualize the input images . First, lets select an image and its label, print the shape. . # Select an image and its label by list index image_index = 0 selected_image = IMAGE_LIST[image_index][0] selected_label = IMAGE_LIST[image_index][1] print(selected_image.shape) print(selected_label) . (458, 800, 3) day . Display day and night images and check the difference between them. . # start with day image # convert it to RGB format selected_copy = np.copy(IMAGE_LIST[image_index][0]) selected_copy = cv2.cvtColor(selected_copy, cv2.COLOR_BGR2RGB) plt.imshow(selected_copy, cmap = &#39;gray&#39;) plt.title(IMAGE_LIST[image_index][1]) . Text(0.5, 1.0, &#39;day&#39;) . . # Select an image and its label by list index image_index = 43 night_image = IMAGE_LIST[image_index][0] night_label = IMAGE_LIST[image_index][1] print(night_image.shape) print(night_label) . (458, 800, 3) night . # pursue with night image # convert it to RGB format night_copy = np.copy(IMAGE_LIST[image_index][0]) night_copy = cv2.cvtColor(night_copy, cv2.COLOR_BGR2RGB) plt.imshow(night_copy, cmap = &#39;gray&#39;) plt.title(IMAGE_LIST[image_index][1]) . Text(0.5, 1.0, &#39;night&#39;) . . Standardize images . Let’s create a class for input standartization. Note, we need to standardize 2 objects: . image; | its label (convert categorical to numerical). | . We will: . resize image with shape (600x1100 px); | convert day to 1 and night to 0; | output a list of standardized images. | . class Standardize(): def __init__(self): # self.image_list = image_list self.standardized_list = [] self.numerical_value = 0 # initialize the function def standardize_input(self, image_list): for item in image_list: # image has index 0 # label has index 1 image = item[0] label = item[1] image = np.copy(image) label = np.copy(label) # start with image resize image_std = cv2.resize(image, (1100, 600)) label_binary = self.numerical_value if label == &#39;night&#39; else 1 self.standardized_list.append((image_std, label_binary)) return self.standardized_list . s1 = Standardize() IMAGE_LIST_stand = s1.standardize_input(IMAGE_LIST) . print(&quot;Length of standardized images dataset:&quot;,len(IMAGE_LIST_stand)) print(&quot;Length of original images dataset:&quot;,len(IMAGE_LIST)) . Length of standardized images dataset: 79 Length of original images dataset: 79 . example_img = IMAGE_LIST_stand[0][0] example_img = np.copy(example_img) plt.imshow(example_img) print(&#39;Image label:&#39;, IMAGE_LIST_stand[0][1]) . Image label: 1 . . Feature extraction . Now, we are ready to separate these images on day and night based on average brightness. This will be a single value and we assume, that average value for day will be higher than average value for night. To calculate average brightness we will use HSV colorspace - Value channel in particular: we will sum it up and divide by area of image (height multiplied by width). . First of all, we will take a look at couple of day and night images. Lets convert them from RGB to HSV. . # fist day image # use standardized list of images we prepared above img_day_1 = IMAGE_LIST_stand[0][0] img_day_1 = np.array(img_day_1) img_day_1 = cv2.cvtColor(img_day_1, cv2.COLOR_RGB2HSV) # second day image img_day_2 = IMAGE_LIST_stand[1][0] img_day_2 = np.array(img_day_2) img_day_2 = cv2.cvtColor(img_day_2, cv2.COLOR_RGB2HSV) . # fist night image img_night_1 = IMAGE_LIST_stand[43][0] img_night_1 = np.array(img_night_1) img_night_1 = cv2.cvtColor(img_night_1, cv2.COLOR_RGB2HSV) # second night image img_night_2 = IMAGE_LIST_stand[53][0] img_night_2 = np.array(img_night_2) img_night_2 = cv2.cvtColor(img_night_2, cv2.COLOR_RGB2HSV) . Below we display day and night images in HSV colorspace. . image_day_1_h = img_day_1[:,:,0] image_day_1_s = img_day_1[:,:,1] image_day_1_v = img_day_1[:,:,2] fig, (ax0, ax1,ax2,ax3) = plt.subplots(1,4, figsize = (20,10)) ax0.imshow(IMAGE_LIST_stand[0][0]) ax0.set_title(&#39;Standardized day image 1&#39;) ax1.imshow(image_day_1_h, cmap = &#39;gray&#39;) ax1.set_title(&#39;Hue, day image 1&#39;) ax2.imshow(image_day_1_s, cmap = &#39;gray&#39;) ax2.set_title(&#39;Saturation, day image 1&#39;) ax3.imshow(image_day_1_v, cmap = &#39;gray&#39;) ax3.set_title(&#39;Value, day image 1&#39;) . Text(0.5, 1.0, &#39;Value, day image 1&#39;) . . image_day_2_h = img_day_2[:,:,0] image_day_2_s = img_day_2[:,:,1] image_day_2_v = img_day_2[:,:,2] fig, (ax0, ax1,ax2,ax3) = plt.subplots(1,4, figsize = (20,10)) ax0.imshow(IMAGE_LIST_stand[1][0]) ax0.set_title(&#39;Standardized day image 2&#39;) ax1.imshow(image_day_2_h, cmap = &#39;gray&#39;) ax1.set_title(&#39;Hue, image 2&#39;) ax2.imshow(image_day_2_s, cmap = &#39;gray&#39;) ax2.set_title(&#39;Saturation, image 2&#39;) ax3.imshow(image_day_2_v, cmap = &#39;gray&#39;) ax3.set_title(&#39;Value, image 2&#39;) . Text(0.5, 1.0, &#39;Value, image 2&#39;) . . Based on days images we can say, that Value channel is high for the skies. . img_night_1_h = img_night_1[:,:,0] img_night_1_s = img_night_1[:,:,1] img_night_1_v = img_night_1[:,:,2] fig, (ax0, ax1,ax2,ax3) = plt.subplots(1,4, figsize = (20,10)) ax0.imshow(IMAGE_LIST_stand[43][0]) ax0.set_title(&#39;Standardized night image 1&#39;) ax1.imshow(img_night_1_h, cmap = &#39;gray&#39;) ax1.set_title(&#39;Hue, night image 1&#39;) ax2.imshow(img_night_1_s, cmap = &#39;gray&#39;) ax2.set_title(&#39;Saturation, night image 1&#39;) ax3.imshow(img_night_1_v, cmap = &#39;gray&#39;) ax3.set_title(&#39;Value, night image 1&#39;) . Text(0.5, 1.0, &#39;Value, night image 1&#39;) . . img_night_2_h = img_night_2[:,:,0] img_night_2_s = img_night_2[:,:,1] img_night_2_v = img_night_2[:,:,2] fig, (ax0, ax1,ax2,ax3) = plt.subplots(1,4, figsize = (20,10)) ax0.imshow(IMAGE_LIST_stand[53][0]) ax0.set_title(&#39;Standardized night image 2&#39;) ax1.imshow(img_night_2_h, cmap = &#39;gray&#39;) ax1.set_title(&#39;Hue, night image 2&#39;) ax2.imshow(img_night_2_s, cmap = &#39;gray&#39;) ax2.set_title(&#39;Saturation, night image 2&#39;) ax3.imshow(img_night_2_v, cmap = &#39;gray&#39;) ax3.set_title(&#39;Value, night image 2&#39;) . Text(0.5, 1.0, &#39;Value, night image 2&#39;) . . Find average brightness using V channel . Write the class that inputs entire list of standardized images and outputs following: . new list with added average brightness to tuple: pair of image, label; | visulalize distribution of night and day averages. | . plt.style.use(&#39;ggplot&#39;) %matplotlib inline . class AverageBrightness(): def __init__(self): # define two functions: add value to pair # and visualize distributions self.list_with_bright = [] self.night_list = [] self.day_list = [] def average_bright(self, standardized_list): for item in standardized_list: rgb_image = item[0] hsv = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2HSV) value = np.sum(hsv[:,:,2]) pxl = hsv.shape[0] * hsv.shape[1] aver_bright = value / pxl # Note: we are adding item to a tuple! We use +(&lt;some item&gt;, ) item = item + (aver_bright,) self.list_with_bright.append(item) return self.list_with_bright # plot resulted average brightness with respect to day/night def plot_brightness(self, list_with_bright): for item in list_with_bright: brightness_value = item[2] if item[1]==0: self.night_list.append(brightness_value) else: self.day_list.append(brightness_value) # plot night average brightness fig, ax1 = plt.subplots(figsize = (10,6)) mean = np.mean(self.night_list) sns.distplot(self.night_list) ax1.axvline(mean, color=&#39;k&#39;, linestyle=&#39;dashed&#39;, linewidth=0.8) ax1.set_title(&#39;Night average brightness distribution&#39;) print(&#39;Night Mean value: &#39;, round(mean,2)) # plot day average brightness fig, ax2 = plt.subplots(figsize = (10,6)) mean = np.mean(self.day_list) sns.distplot(self.day_list) ax2.axvline(mean, color=&#39;k&#39;, linestyle=&#39;dashed&#39;, linewidth=0.8) ax2.set_title(&#39;Day average brightness distribution&#39;) print(&#39;Day Mean value: &#39;, round(mean,2)) . s1 = AverageBrightness() full_list = s1.average_bright(IMAGE_LIST_stand) . print(&#39;Standardized list length:&#39;, len(IMAGE_LIST_stand)) print(&#39;Embedded list length:&#39;, len(full_list)) . Standardized list length: 79 Embedded list length: 79 . s1.plot_brightness(full_list) . Night Mean value: 46.87 Day Mean value: 151.35 . . . Next, our step will be to look at average brightness value for day and night images. Our goal is to find a value (threshold), that clearly separates day and night. . Classification . Create predicted_label that will turn 1 for a day image and 0 for night image. We will build estimate_label function that will input average brightness of an image and output an estimated label based on threshold. Let’s set threshold value = 110 for this classifier. . Next we write a class called SimpleClassifier with input: . standardized list; | . and output: . accuracy; | list of true/predicted labels. | . # use RGB image as an output def estimate_label(rgb_image): avg = average_brightness(rgb_image) predicted_label = 0 threshold = 110 predicted_label = 1 if avg &gt;=threshold else predicted_label return predicted_label . &quot;&quot;&quot; - convert to HSV colorspace - sum up by Value - divide resulted sum by number of pixels &quot;&quot;&quot; def average_brightness(rgb_image): rgb_image = np.copy(rgb_image) image_hsv = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2HSV) # add up all the pixel values in V channel value = np.sum(image_hsv[:,:, 2]) # multiply height and width pxl = image_hsv.shape[0] * image_hsv.shape[1] average_value = round(value/pxl,3) return average_value . &quot;&quot;&quot; - Create a classifier object; - Recall, each item in standardized image list consists of: a) item[0] - image itself; b) item[1] - true label; c) item[2] - average brightness value; d) item[3] - predicted label. &quot;&quot;&quot; class SimpleClassifier(): def __init__(self): self.predictions = [] def predict_label(self, standardized_list, threshold): s1 = AverageBrightness() full_list = s1.average_bright(standardized_list) for item in full_list: brightness_value = item[2] if brightness_value &gt;= threshold: # will return 1 or 0 (day or night) item = item + (1,) else: item = item + (0,) self.predictions.append((item[1], item[3])) # check how many labels are classified right confusion_list = [i[0]==i[1] for i in self.predictions] accuracy = sum(confusion_list) / len(confusion_list) return self.predictions, round(accuracy, 2) . s2 = SimpleClassifier() estimated_labels_list = s2.predict_label(IMAGE_LIST_stand, threshold = 110) . print(&#39;Accuracy of Simple Classifier: &#39;,estimated_labels_list[1]) . Accuracy of Simple Classifier: 0.85 . Testing classifier . Since we are using simple brightness as a feature, we do not expect a high accuracy for our classifier. We aim to get 75-85% of accuracy using a single feature. . Loading the test data, we standardize and shuffle it: last ensures that the order of our data will not play a role in testing accuracy. . &quot;&quot;&quot; - lead test dataset and apply standardize_input() function, that we created previously - shuffle data using random function &quot;&quot;&quot; import random s1 = Standardize() TEST_IMAGE_LIST = helpers.load_dataset(image_dir_test) TEST_IMAGE_LIST_std = s1.standardize_input(TEST_IMAGE_LIST) random.shuffle(TEST_IMAGE_LIST_std) . Let’s rewrite classifier in functional form. We will iterate through the test data images and add misclassified images and their labels into misclassified_images_lables list. . def get_misclassified_labels(test_images): # create an empty list, whre we will pass images and corresponding labels misclassified_images_lables = [] # then we iterate through images # classify them and compare to the labels for item in test_images: img = item[0] true_label = item[1] # apply created function predicted_label = estimate_label(img) # compare labels and add them to misclassified_list if (predicted_label != true_label): misclassified_images_lables.append((img, predicted_label, true_label)) return misclassified_images_lables . mis_classed_list = get_misclassified_labels(TEST_IMAGE_LIST_std) total_images = len(TEST_IMAGE_LIST_std) misclass = len(mis_classed_list) print(&#39;Accuracy:&#39;, (total_images-misclass)/total_images) . Accuracy: 1.0 . .",
            "url": "https://machinelearninghelsinki.github.io/mlstudygroup/learning/2020/09/12/Intro-to-Image-processing-part-1.html",
            "relUrl": "/learning/2020/09/12/Intro-to-Image-processing-part-1.html",
            "date": " • Sep 12, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Intro To Image Processing Part 1",
            "content": "&lt;!DOCTYPE html&gt;&lt;html lang=&quot;zh-cmn-Hans&quot;&gt;&lt;head&gt; . &lt;/head&gt; . . toc: true layout: post description: Image processing with OpenCV categories: [learning] comments: true . title: &quot;Image processing with OpenCV: Part 1&quot; . Image processing with OpenCV: Part 1 . Disclaimer . The notebook is prepared on initial repo: https://github.com/MakarovArtyom/OpenCV-Python-practices . Acknowledgement . The code from Jupyter notebooks follows Udacity Computer Vision nanodegree logic. More Computer Vision exercises can be found under Udacity CVND repo: https://github.com/udacity/CVND_Exercises . Libraries . # uncomment in case PyQt5 is not installed #! pip install PyQt5 . import numpy as np import matplotlib.pyplot as plt # for image reading import matplotlib.image as mpimg import cv2 # makes image pop-up in interactive window %matplotlib qt . Image reading . Given an RGB image, let&apos;s read it using matplotlib (mpimg) . . The RGB color model is an additive color model in which red, green, and blue light are added together in various ways to reproduce a broad array of colors. The name of the model comes from the initials of the three additive primary colors, red, green, and blue . Source: https://en.wikipedia.org/wiki/RGB_color_model . image = mpimg.imread(&apos;images/robot_blue.jpg&apos;) . print(&apos;Shape of the image:&apos;, image.shape) . Shape of the image: (720, 1280, 3) . Per single image, we can retrieve the following info: . height = 720 | width = 1280 | number of channels = 3 | . Later on, we will see that for many &quot;classical&quot; computer vision algorithms the color information is redundant. For solving problems like edge, corner or blob detection, the grayscale image is enough. . Let&apos;s use cvtColor function to convert RGB image to gray. . gray_img = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) plt.imshow(gray_img, cmap = &apos;gray&apos;) . &lt;matplotlib.image.AxesImage at 0x7fdd7c819828&gt; . ![](/mlstudygroup/images/output_11_1.png &quot;rgb to gray&quot;) . print(&apos;Shape of the image after convertation:&apos;, gray_img.shape) . Shape of the image after convertation: (720, 1280) . It&apos;s important to understand an image is just a matrix (tensor) with values, representing the intensity of pixels (from 0 to 255). Each pixel has it&apos;s own coordinates. . print(&apos;Max pixel value:&apos;, gray_img.max()) print(&apos;Min pixel value:&apos;, gray_img.min()) . Max pixel value: 255 Min pixel value: 0 . Use the pair of (x,y) coordinates to access particular pixel&apos;s value. . x = 45 y = 52 pixel_value = gray_img[x,y] print(pixel_value) . 28 . Note, it&apos;s possible to manipulate pixel values by scalar multiplication and augmentation in different formats. . Example: let&apos;s create a 6x6 image with random pixel values. . pixel = abs(np.random.randn(6,6)) * 10 pixel.astype(int) . array([[15, 8, 3, 14, 3, 3], [ 6, 9, 10, 7, 0, 5], [11, 18, 10, 4, 14, 5], [ 9, 6, 0, 1, 1, 16], [18, 9, 6, 29, 8, 7], [12, 0, 2, 0, 9, 12]]) . Next, display the results with grayscale colormap. . plt.matshow(pixel, cmap = &apos;gray&apos;) . &lt;matplotlib.image.AxesImage at 0x7fdd7acd96d8&gt; . Color isolating . %matplotlib inline . Let&apos;s read an image and see how we can visualize 3 RBG colors individually. . img = mpimg.imread(&apos;images/car.jpeg&apos;) plt.imshow(img) . &lt;matplotlib.image.AxesImage at 0x7feaec8de0b8&gt; . ![](/mlstudygroup/images/open_cv/output_24_1.png) . print(&apos;Shape of the image:&apos;, img.shape) . Shape of the image: (281, 500, 3) . Since the color image is a tensor composed of 3 colors, we can express &quot;red&quot;, &quot;green&quot; and &quot;bleu&quot; as follows. . r = img[:,:, 0] # red g = img[:,:, 1] # green b = img[:,:, 2] # blue print(&apos;Shape of individual color matrix:&apos;, r.shape) . Shape of individual color matrix: (281, 500) . f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20,10)) # visualize red ax1.set_title(&apos;R channel&apos;) ax1.imshow(r, cmap = &apos;gray&apos;) # green ax2.set_title(&apos;G channel&apos;) ax2.imshow(g, cmap = &apos;gray&apos;) # blue ax3.set_title(&apos;B channel&apos;) ax3.imshow(b, cmap = &apos;gray&apos;) . &lt;matplotlib.image.AxesImage at 0x7feb0295d550&gt; . ![](/mlstudygroup/images/open_cv/output_28_1.png) . Background change . Isolating object&apos;s background . Additional resources: . Color picker to choose boundaries: https://www.w3schools.com/colors/colors_picker.asp | . Suppose we have an image of an object on solid color background. Consider the image of a robot below - we read it with open-cv library. . img = cv2.imread(&apos;images/robot_blue.jpg&apos;) plt.imshow(img) . &lt;matplotlib.image.AxesImage at 0x7feaec56f6d8&gt; . Important note: open-cv reads images as BGR, not RGB - as a result the output is different from original image. In particular - red and blue colors are in reverse order. . Next, we take a copy of image and change it from BGR to RGB (pass parameter cv2.COLOR_BGR2RGB). Note, that any transformations applied to a copy will not effect an image. . &quot;&quot;&quot; - make a copy with numpy - change from BGR to RGB &quot;&quot;&quot; img_copy = np.copy(img) img_copy = cv2.cvtColor(img_copy, cv2.COLOR_BGR2RGB) plt.imshow(img_copy) . &lt;matplotlib.image.AxesImage at 0x7feaec6dc4e0&gt; . ![](/mlstudygroup/images/open_cv/output_36_1.png) . First, define color a threshold. It will be a lower and upper bounds for the background I wnat to isolate. Here we need to specify 3 values - for each color - red, green and blue. . Lower bound: red and green as zero, and high value for blue. For example, 230. | Upper bound: red, green - some small values and blue - maximum, i.e. 250. So, allow a bit red and green. | . All values within this range will be considered as intense blue color. This range will be replaced by another background. . blue_lower = np.array([0, 0, 230]) blue_upper = np.array([50, 50, 250]) . Creating mask . We are going to use a common way to isolate chosen area - creating a mask. Now, we will isolate blue background area using inRange() function. It verifies each pixel whether it falls into specific range, defined by lower and upper bounds. . If it falls - the white mask will be displayed; | If it does not fall within the range - pixel will be turned into black. | . Simply saying, everything inside the interval will be white. . &quot;&quot;&quot; - function will take lower and upper bounds of image - and define the mask &quot;&quot;&quot; mask = cv2.inRange(img_copy, blue_lower, blue_upper) plt.imshow(mask, cmap = &apos;gray&apos;) . &lt;matplotlib.image.AxesImage at 0x7feaec74afd0&gt; . ![](/mlstudygroup/images/open_cv/output_41_1.png) . White area - area, where the new image background will be displayed. | Black area - where ew image background will be blocked out. | . The mask has the same height and width, and each pixel has 2 values: . 255 - for white area; | 0 - for black area. | . Now, let&apos;s put our object into a mask. . First of all, create a copy of image (just if we want to change it later on). | Then, one way to separate blue background from this is to check where blue pixels intersect with mask white pixels, or in other words - do not intersect with black (not equal 0). We will set them to black color. | . Displaying an image, we will see that only object appears on a black background. . masked_img = np.copy(img_copy) masked_img[mask!=0] = [0,0,0] plt.imshow(masked_img) . &lt;matplotlib.image.AxesImage at 0x7feaec47eba8&gt; . ![](/mlstudygroup/images/open_cv/output_46_1.png) . Applying new background . Next step will be just to apply new background on top of black one. We just take an image, (e.g. &quot;space image&quot; below and convert it from BGR to RGB). . background_img = cv2.imread(&apos;images/background_img.jpg&apos;) background_img = cv2.cvtColor(background_img, cv2.COLOR_BGR2RGB) plt.imshow(background_img) . &lt;matplotlib.image.AxesImage at 0x7feaec4e4630&gt; . ![](/mlstudygroup/images/open_cv/output_49_1.png) . It&apos;s also important to crop background image to make it the same size as our robot image. We crop it by height and width. . &quot;&quot;&quot; - cropping to a size of (720, 1280) &quot;&quot;&quot; crop_background_img = background_img[0:720, 0:1280] . Now, we will do an opposite operation with a cropped background image: choose pixels that are equal to 0 in mask image (there we had black robot) and set these pixels to a black on cropped background image. . Simply saying, we merge mask and crop_background_img. . crop_background_img[mask == 0] = [0,0,0] plt.imshow(crop_background_img) . &lt;matplotlib.image.AxesImage at 0x7feaeb3c8278&gt; . ![](/mlstudygroup/images/open_cv/output_53_1.png) . Final step: add object and new background together . Since the black area on cropped background image is equivalent to 0, we can add this image to masked image. In this case simple summation will work, since we deal with matrices. . final_image = crop_background_img + masked_img plt.imshow(final_image) . &lt;matplotlib.image.AxesImage at 0x7feaeb425be0&gt; . ![](/mlstudygroup/images/open_cv/output_56_1.png) . Object detection in HSV space . HSV colorspace . HSV colorspace is similar to RGB and also represents 3 channel - hue, saturation and value. This space is commonly used in tasks like image segmentation. In HSV colorspace Hue channel models color type, while Saturation and Value represents color as a mixture of shades and brightness. . Since the hue channel models the color type, it is very useful in image processing tasks that need to segment objects based on its color. Variation of the saturation goes from unsaturated to represent shades of gray and fully saturated (no white component). Value channel describes the brightness or the intensity of the color. . Source: https://docs.opencv.org/3.4/da/d97/tutorial_threshold_inRange.html . We will start with image displaying. We will use both RGB and HSV color spaces to detect the balloons in the image. . img = cv2.imread(&apos;images/baloons.jpg&apos;) img_copy = np.copy(img) img_copy = cv2.cvtColor(img_copy, cv2.COLOR_BGR2RGB) plt.imshow(img_copy) . &lt;matplotlib.image.AxesImage at 0x7feaef450a58&gt; . ![](/mlstudygroup/images/open_cv/output_62_1.png) . First, let&apos;s plot the color channels in RGB space. . # RGB channels # we take all pixels, but isolate the color we need red = img_copy[:,:,0] green = img_copy[:,:,1] blue = img_copy[:,:,2] # now, plot each of this colors in gray scale to see the relative intensities f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize = (20,10)) ax1.set_title(&apos;Red scale&apos;) ax1.imshow(red, cmap = &apos;gray&apos;) ax2.set_title(&apos;Green scale&apos;) ax2.imshow(green, cmap = &apos;gray&apos;) ax3.set_title(&apos;Blue scale&apos;) ax3.imshow(blue, cmap = &apos;gray&apos;) . &lt;matplotlib.image.AxesImage at 0x7feaefa82630&gt; . ![](/mlstudygroup/images/open_cv/output_64_1.png) . We can see that pink balloons have high values for the red (close to 255, white color) and medium values for blue. However, there are many variations, especially if the balloon is in shadow. . Next, we convert image from RGB to HSV. . hsv = cv2.cvtColor(img_copy, cv2.COLOR_RGB2HSV) . Now, we isolate each of these channels as we did before. . hue = hsv[:,:,0] saturation = hsv[:,:,1] value = hsv[:,:,2] f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize = (20,10)) ax1.set_title(&apos;Hue&apos;) ax1.imshow(hue, cmap = &apos;gray&apos;) ax2.set_title(&apos;Saturation&apos;) ax2.imshow(saturation, cmap = &apos;gray&apos;) ax3.set_title(&apos;Value&apos;) ax3.imshow(value, cmap = &apos;gray&apos;) . &lt;matplotlib.image.AxesImage at 0x7feaf0b542b0&gt; . ![](/mlstudygroup/images/open_cv/output_69_1.png) . Compare this to our image. Take a look at pink balloons: . Hue has high values for pink balloons, and even in shadow hue level is pretty consistent; | Saturation values are quite versatile depending on shadow. | . Create thresholds for pink color in RGB and HSV . Now, create lower and upper bounds for pink color in RGB and then in HSV spaces. . # use thrashold for pink color in RGB # link to check: https://www.w3schools.com/colors/colors_rgb.asp lower_pink = np.array([180,0,100]) upper_pink = np.array([255,255,230]) . Then, do the same for HSV. Remember, that hue goes from 0 to 180 degrees. We will limit this from 160 to 180. Will allow any values for saturation and value from 0 to 255. . lower_hue = np.array([140,0,0]) upper_hue = np.array([180,255,255]) . Mask the images . Now, we will create a mask to see how well these thresholds select the pink balloons. As previous we will use inRage() function and apply mask afterwards. . &quot;&quot;&quot; - first we create mask based on lower and upper bounds. Mask will assign 0 to all pixels that are out of these interval. - then we make all pixels, intersecting with black color (have 0 values) black (assign to [0,0,0]). - isolate pink color &quot;&quot;&quot; mask_rgb = cv2.inRange(img_copy, lower_pink, upper_pink) masked_image = np.copy(img_copy) masked_image[mask_rgb == 0] = [0,0,0] plt.imshow(masked_image) plt.title(&apos;RGB selection&apos;) . Text(0.5, 1.0, &apos;RGB selection&apos;) . ![](/mlstudygroup/images/open_cv/output_78_1.png) . Here we can see RGB colorspace: . Does not select all pink balloons (or ones that are in shadow); | Selects some other colors. | . Finally, repeat the procedure for HSV colorspace. . &quot;&quot;&quot; - note, we will use hsv image (RGB2HSV) &quot;&quot;&quot; mask_hsv = cv2.inRange( hsv, lower_hue, upper_hue) masked_image = np.copy(hsv) masked_image[mask_hsv == 0] = [0,0,0] plt.imshow(masked_image) plt.title(&apos;HSV selection&apos;) . Text(0.5, 1.0, &apos;HSV selection&apos;) . ![](/mlstudygroup/images/open_cv/output_81_1.png) . Day and Night Image Classifier . The day/night image dataset consists of 200 RGB color images in two categories: day and night. There are equal numbers of each example: 100 day images and 100 night images. . We&apos;d like to build a classifier that can accurately label these images as day or night, and that relies on finding distinguishing features between the two types of images! . Note: All images come from the AMOS dataset (Archive of Many Outdoor Scenes). . Training and Testing Data . The 200 day/night images are separated into training and testing datasets. . 60% of these images are training images, for you to use as you create a classifier. | 40% are test images, which will be used to test the accuracy of your classifier. | . First, we set some variables to keep track of some where our images are stored: . `image_dir_training`: the directory where our training image data is stored `image_dir_test`: the directory where our test image data is stored . # Image data directories image_dir_training = &quot;training&quot; image_dir_test = &quot;test&quot; . Load the datasets . These first few lines of code will load the training day/night images and store all of them in a variable, IMAGE_LIST. This list contains the images and their associated label (&quot;day&quot; or &quot;night&quot;). . For example, the first image-label pair in IMAGE_LIST can be accessed by index: . python # Using the load_dataset function in helpers.py # Load training data IMAGE_LIST = helpers.load_dataset(image_dir_training) . Train dataset contains: . Images, classified as &quot;day&quot; or &quot;night&quot;; | It&apos;s possible to access first pair &quot;image-label&quot; by index 0 - IMAGE_LIST[0] | To access label of e.g. first image - IMAGE_LIST[0][0]. | . Visualize the input images . First, lets select an image and its label, print the shape. . # Select an image and its label by list index image_index = 0 selected_image = IMAGE_LIST[image_index][0] selected_label = IMAGE_LIST[image_index][1] print(selected_image.shape) print(selected_label) . (458, 800, 3) day . Display day and night images and check the difference between them. . # start with day image # convert it to RGB format selected_copy = np.copy(IMAGE_LIST[image_index][0]) selected_copy = cv2.cvtColor(selected_copy, cv2.COLOR_BGR2RGB) plt.imshow(selected_copy, cmap = &apos;gray&apos;) plt.title(IMAGE_LIST[image_index][1]) . Text(0.5, 1.0, &apos;day&apos;) . ![](/mlstudygroup/images/open_cv/output_93_1.png) . # Select an image and its label by list index image_index = 43 night_image = IMAGE_LIST[image_index][0] night_label = IMAGE_LIST[image_index][1] print(night_image.shape) print(night_label) . (458, 800, 3) night . # pursue with night image # convert it to RGB format night_copy = np.copy(IMAGE_LIST[image_index][0]) night_copy = cv2.cvtColor(night_copy, cv2.COLOR_BGR2RGB) plt.imshow(night_copy, cmap = &apos;gray&apos;) plt.title(IMAGE_LIST[image_index][1]) . Text(0.5, 1.0, &apos;night&apos;) . ![](/mlstudygroup/images/open_cv/output_95_1.png) . Standardize images . Let&apos;s create a class for input standartization. Note, we need to standardize 2 objects: . image; | its label (convert categorical to numerical). | . We will: . resize image with shape (600x1100 px); | convert day to 1 and night to 0; | output a list of standardized images. | . class Standardize(): def __init__(self): # self.image_list = image_list self.standardized_list = [] self.numerical_value = 0 # initialize the function def standardize_input(self, image_list): for item in image_list: # image has index 0 # label has index 1 image = item[0] label = item[1] image = np.copy(image) label = np.copy(label) # start with image resize image_std = cv2.resize(image, (1100, 600)) label_binary = self.numerical_value if label == &apos;night&apos; else 1 self.standardized_list.append((image_std, label_binary)) return self.standardized_list . s1 = Standardize() IMAGE_LIST_stand = s1.standardize_input(IMAGE_LIST) . print(&quot;Length of standardized images dataset:&quot;,len(IMAGE_LIST_stand)) print(&quot;Length of original images dataset:&quot;,len(IMAGE_LIST)) . Length of standardized images dataset: 79 Length of original images dataset: 79 . example_img = IMAGE_LIST_stand[0][0] example_img = np.copy(example_img) plt.imshow(example_img) print(&apos;Image label:&apos;, IMAGE_LIST_stand[0][1]) . Image label: 1 . ![](/mlstudygroup/images/open_cv/output_101_1.png) . Feature extraction . Now, we are ready to separate these images on day and night based on average brightness. This will be a single value and we assume, that average value for day will be higher than average value for night. To calculate average brightness we will use HSV colorspace - Value channel in particular: we will sum it up and divide by area of image (height multiplied by width). . First of all, we will take a look at couple of day and night images. Lets convert them from RGB to HSV. . # fist day image # use standardized list of images we prepared above img_day_1 = IMAGE_LIST_stand[0][0] img_day_1 = np.array(img_day_1) img_day_1 = cv2.cvtColor(img_day_1, cv2.COLOR_RGB2HSV) # second day image img_day_2 = IMAGE_LIST_stand[1][0] img_day_2 = np.array(img_day_2) img_day_2 = cv2.cvtColor(img_day_2, cv2.COLOR_RGB2HSV) . # fist night image img_night_1 = IMAGE_LIST_stand[43][0] img_night_1 = np.array(img_night_1) img_night_1 = cv2.cvtColor(img_night_1, cv2.COLOR_RGB2HSV) # second night image img_night_2 = IMAGE_LIST_stand[53][0] img_night_2 = np.array(img_night_2) img_night_2 = cv2.cvtColor(img_night_2, cv2.COLOR_RGB2HSV) . Below we display day and night images in HSV colorspace. . image_day_1_h = img_day_1[:,:,0] image_day_1_s = img_day_1[:,:,1] image_day_1_v = img_day_1[:,:,2] fig, (ax0, ax1,ax2,ax3) = plt.subplots(1,4, figsize = (20,10)) ax0.imshow(IMAGE_LIST_stand[0][0]) ax0.set_title(&apos;Standardized day image 1&apos;) ax1.imshow(image_day_1_h, cmap = &apos;gray&apos;) ax1.set_title(&apos;Hue, day image 1&apos;) ax2.imshow(image_day_1_s, cmap = &apos;gray&apos;) ax2.set_title(&apos;Saturation, day image 1&apos;) ax3.imshow(image_day_1_v, cmap = &apos;gray&apos;) ax3.set_title(&apos;Value, day image 1&apos;) . Text(0.5, 1.0, &apos;Value, day image 1&apos;) . ![](/mlstudygroup/images/open_cv/output_108_1.png) . image_day_2_h = img_day_2[:,:,0] image_day_2_s = img_day_2[:,:,1] image_day_2_v = img_day_2[:,:,2] fig, (ax0, ax1,ax2,ax3) = plt.subplots(1,4, figsize = (20,10)) ax0.imshow(IMAGE_LIST_stand[1][0]) ax0.set_title(&apos;Standardized day image 2&apos;) ax1.imshow(image_day_2_h, cmap = &apos;gray&apos;) ax1.set_title(&apos;Hue, image 2&apos;) ax2.imshow(image_day_2_s, cmap = &apos;gray&apos;) ax2.set_title(&apos;Saturation, image 2&apos;) ax3.imshow(image_day_2_v, cmap = &apos;gray&apos;) ax3.set_title(&apos;Value, image 2&apos;) . Text(0.5, 1.0, &apos;Value, image 2&apos;) . ![](/mlstudygroup/images/open_cv/output_109_1.png) . Based on days images we can say, that Value channel is high for the skies. . img_night_1_h = img_night_1[:,:,0] img_night_1_s = img_night_1[:,:,1] img_night_1_v = img_night_1[:,:,2] fig, (ax0, ax1,ax2,ax3) = plt.subplots(1,4, figsize = (20,10)) ax0.imshow(IMAGE_LIST_stand[43][0]) ax0.set_title(&apos;Standardized night image 1&apos;) ax1.imshow(img_night_1_h, cmap = &apos;gray&apos;) ax1.set_title(&apos;Hue, night image 1&apos;) ax2.imshow(img_night_1_s, cmap = &apos;gray&apos;) ax2.set_title(&apos;Saturation, night image 1&apos;) ax3.imshow(img_night_1_v, cmap = &apos;gray&apos;) ax3.set_title(&apos;Value, night image 1&apos;) . Text(0.5, 1.0, &apos;Value, night image 1&apos;) . ![](/mlstudygroup/images/open_cv/output_111_1.png) . img_night_2_h = img_night_2[:,:,0] img_night_2_s = img_night_2[:,:,1] img_night_2_v = img_night_2[:,:,2] fig, (ax0, ax1,ax2,ax3) = plt.subplots(1,4, figsize = (20,10)) ax0.imshow(IMAGE_LIST_stand[53][0]) ax0.set_title(&apos;Standardized night image 2&apos;) ax1.imshow(img_night_2_h, cmap = &apos;gray&apos;) ax1.set_title(&apos;Hue, night image 2&apos;) ax2.imshow(img_night_2_s, cmap = &apos;gray&apos;) ax2.set_title(&apos;Saturation, night image 2&apos;) ax3.imshow(img_night_2_v, cmap = &apos;gray&apos;) ax3.set_title(&apos;Value, night image 2&apos;) . Text(0.5, 1.0, &apos;Value, night image 2&apos;) . ![](/mlstudygroup/images/open_cv/output_112_1.png) . Find average brightness using V channel . Write the class that inputs entire list of standardized images and outputs following: . new list with added average brightness to tuple: pair of image, label; | visulalize distribution of night and day averages. | . plt.style.use(&apos;ggplot&apos;) %matplotlib inline . class AverageBrightness(): def __init__(self): # define two functions: add value to pair # and visualize distributions self.list_with_bright = [] self.night_list = [] self.day_list = [] def average_bright(self, standardized_list): for item in standardized_list: rgb_image = item[0] hsv = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2HSV) value = np.sum(hsv[:,:,2]) pxl = hsv.shape[0] * hsv.shape[1] aver_bright = value / pxl # Note: we are adding item to a tuple! We use +(&lt;some item&gt;, ) item = item + (aver_bright,) self.list_with_bright.append(item) return self.list_with_bright # plot resulted average brightness with respect to day/night def plot_brightness(self, list_with_bright): for item in list_with_bright: brightness_value = item[2] if item[1]==0: self.night_list.append(brightness_value) else: self.day_list.append(brightness_value) # plot night average brightness fig, ax1 = plt.subplots(figsize = (10,6)) mean = np.mean(self.night_list) sns.distplot(self.night_list) ax1.axvline(mean, color=&apos;k&apos;, linestyle=&apos;dashed&apos;, linewidth=0.8) ax1.set_title(&apos;Night average brightness distribution&apos;) print(&apos;Night Mean value: &apos;, round(mean,2)) # plot day average brightness fig, ax2 = plt.subplots(figsize = (10,6)) mean = np.mean(self.day_list) sns.distplot(self.day_list) ax2.axvline(mean, color=&apos;k&apos;, linestyle=&apos;dashed&apos;, linewidth=0.8) ax2.set_title(&apos;Day average brightness distribution&apos;) print(&apos;Day Mean value: &apos;, round(mean,2)) . s1 = AverageBrightness() full_list = s1.average_bright(IMAGE_LIST_stand) . print(&apos;Standardized list length:&apos;, len(IMAGE_LIST_stand)) print(&apos;Embedded list length:&apos;, len(full_list)) . Standardized list length: 79 Embedded list length: 79 . s1.plot_brightness(full_list) . Night Mean value: 46.87 Day Mean value: 151.35 . ![](/mlstudygroup/images/open_cv/output_119_1.png) . ![](/mlstudygroup/images/open_cv/output_119_2.png) . Next, our step will be to look at average brightness value for day and night images. Our goal is to find a value (threshold), that clearly separates day and night. . Classification . Create predicted_label that will turn 1 for a day image and 0 for night image. We will build estimate_label function that will input average brightness of an image and output an estimated label based on threshold. Let&apos;s set threshold value = 110 for this classifier. . Next we write a class called SimpleClassifier with input: . standardized list; | . and output: . accuracy; | list of true/predicted labels. | . # use RGB image as an output def estimate_label(rgb_image): avg = average_brightness(rgb_image) predicted_label = 0 threshold = 110 predicted_label = 1 if avg &gt;=threshold else predicted_label return predicted_label . &quot;&quot;&quot; - convert to HSV colorspace - sum up by Value - divide resulted sum by number of pixels &quot;&quot;&quot; def average_brightness(rgb_image): rgb_image = np.copy(rgb_image) image_hsv = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2HSV) # add up all the pixel values in V channel value = np.sum(image_hsv[:,:, 2]) # multiply height and width pxl = image_hsv.shape[0] * image_hsv.shape[1] average_value = round(value/pxl,3) return average_value . &quot;&quot;&quot; - Create a classifier object; - Recall, each item in standardized image list consists of: a) item[0] - image itself; b) item[1] - true label; c) item[2] - average brightness value; d) item[3] - predicted label. &quot;&quot;&quot; class SimpleClassifier(): def __init__(self): self.predictions = [] def predict_label(self, standardized_list, threshold): s1 = AverageBrightness() full_list = s1.average_bright(standardized_list) for item in full_list: brightness_value = item[2] if brightness_value &gt;= threshold: # will return 1 or 0 (day or night) item = item + (1,) else: item = item + (0,) self.predictions.append((item[1], item[3])) # check how many labels are classified right confusion_list = [i[0]==i[1] for i in self.predictions] accuracy = sum(confusion_list) / len(confusion_list) return self.predictions, round(accuracy, 2) . s2 = SimpleClassifier() estimated_labels_list = s2.predict_label(IMAGE_LIST_stand, threshold = 110) . print(&apos;Accuracy of Simple Classifier: &apos;,estimated_labels_list[1]) . Accuracy of Simple Classifier: 0.85 . Testing classifier . Since we are using simple brightness as a feature, we do not expect a high accuracy for our classifier. We aim to get 75-85% of accuracy using a single feature. . Loading the test data, we standardize and shuffle it: last ensures that the order of our data will not play a role in testing accuracy. . &quot;&quot;&quot; - lead test dataset and apply standardize_input() function, that we created previously - shuffle data using random function &quot;&quot;&quot; import random s1 = Standardize() TEST_IMAGE_LIST = helpers.load_dataset(image_dir_test) TEST_IMAGE_LIST_std = s1.standardize_input(TEST_IMAGE_LIST) random.shuffle(TEST_IMAGE_LIST_std) . Let&apos;s rewrite classifier in functional form. We will iterate through the test data images and add misclassified images and their labels into misclassified_images_lables list. . def get_misclassified_labels(test_images): # create an empty list, whre we will pass images and corresponding labels misclassified_images_lables = [] # then we iterate through images # classify them and compare to the labels for item in test_images: img = item[0] true_label = item[1] # apply created function predicted_label = estimate_label(img) # compare labels and add them to misclassified_list if (predicted_label != true_label): misclassified_images_lables.append((img, predicted_label, true_label)) return misclassified_images_lables . mis_classed_list = get_misclassified_labels(TEST_IMAGE_LIST_std) total_images = len(TEST_IMAGE_LIST_std) misclass = len(mis_classed_list) print(&apos;Accuracy:&apos;, (total_images-misclass)/total_images) . Accuracy: 1.0 . &lt;/html&gt; .",
            "url": "https://machinelearninghelsinki.github.io/mlstudygroup/2020/09/12/Intro-to-Image-processing-part-1.html",
            "relUrl": "/2020/09/12/Intro-to-Image-processing-part-1.html",
            "date": " • Sep 12, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Meetup Sept 05 2020 (Notes)",
            "content": "Notes . Links shared . Link Description . AI in medicine | Coursera link for AI in Health Care | . Deep RL | Deep RL course page | . Some relevant Youtube resources . Deep RL course Emma Brunskill . Deep RL course Pieter Abbeel . Footnotes .",
            "url": "https://machinelearninghelsinki.github.io/mlstudygroup/meetup/2020/09/05/MLMeetupNotes.html",
            "relUrl": "/meetup/2020/09/05/MLMeetupNotes.html",
            "date": " • Sep 5, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "fastcore: An Underrated Python Library",
            "content": ". Background . I recently embarked on a journey to sharpen my python skills: I wanted to learn advanced patterns, idioms, and techniques. I started with reading books on advanced Python, however, the information didn&#39;t seem to stick without having somewhere to apply it. I also wanted the ability to ask questions from an expert while I was learning -- which is an arrangement that is hard to find! That&#39;s when it occurred to me: What if I could find an open source project that has fairly advanced python code and write documentation and tests? I made a bet that if I did this it would force me to learn everything very deeply, and the maintainers would be appreciative of my work and be willing to answer my questions. . And that&#39;s exactly what I did over the past month! I&#39;m pleased to report that it has been the most efficient learning experience I&#39;ve ever experienced. I&#39;ve discovered that writing documentation forced me to deeply understand not just what the code does but also why the code works the way it does, and to explore edge cases while writing tests. Most importantly, I was able to ask questions when I was stuck, and maintainers were willing to devote extra time knowing that their mentorship was in service of making their code more accessible! It turns out the library I choose, fastcore is some of the most fascinating Python I have ever encountered as its purpose and goals are fairly unique. . For the uninitiated, fastcore is a library on top of which many fast.ai projects are built on. Most importantly, fastcore extends the python programming language and strives to eliminate boilerplate and add useful functionality for common tasks. In this blog post, I&#39;m going to highlight some of my favorite tools that fastcore provides, rather than sharing what I learned about python. My goal is to pique your interest in this library, and hopefully motivate you to check out the documentation after you are done to learn more! . Why fastcore is interesting . Get exposed to ideas from other languages without leaving python: I’ve always heard that it is beneficial to learn other languages in order to become a better programmer. From a pragmatic point of view, I’ve found it difficult to learn other languages because I could never use them at work. Fastcore extends python to include patterns found in languages as diverse as Julia, Ruby and Haskell. Now that I understand these tools I am motivated to learn other languages. | You get a new set of pragmatic tools: fastcore includes utilities that will allow you to write more concise expressive code, and perhaps solve new problems. | Learn more about the Python programming language: Because fastcore extends the python programming language, many advanced concepts are exposed during the process. For the motivated, this is a great way to see how many of the internals of python work. | A whirlwind tour through fastcore . Here are some things you can do with fastcore that immediately caught my attention. . . Making **kwargs transparent . Whenever I see a function that has the argument **kwargs, I cringe a little. This is because it means the API is obfuscated and I have to read the source code to figure out what valid parameters might be. Consider the below example: . def baz(a, b=2, c =3, d=4): return a + b + c def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, **kwargs)&gt; . Without reading the source code, it might be hard for me to know that foo also accepts and additional parameters b and d. We can fix this with delegates: . def baz(a, b=2, c =3, d=4): return a + b + c @delegates(baz) # this decorator will pass down keyword arguments from baz def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, b=2, d=4)&gt; . You can customize the behavior of this decorator. For example, you can have your cake and eat it too by passing down your arguments and also keeping **kwargs: . @delegates(baz, keep=True) def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, b=2, d=4, **kwargs)&gt; . You can also exclude arguments. For example, we exclude argument d from delegation: . def basefoo(a, b=2, c =3, d=4): pass @delegates(basefoo, but= [&#39;d&#39;]) # exclude `d` def foo(c, a, **kwargs): pass inspect.signature(foo) . &lt;Signature (c, a, b=2)&gt; . You can also delegate between classes: . class BaseFoo: def __init__(self, e, c=2): pass @delegates()# since no argument was passsed here we delegate to the superclass class Foo(BaseFoo): def __init__(self, a, b=1, **kwargs): super().__init__(**kwargs) inspect.signature(Foo) . &lt;Signature (a, b=1, c=2)&gt; . For more information, read the docs on delegates. . . Avoid boilerplate when setting instance attributes . Have you ever wondered if it was possible to avoid the boilerplate involved with setting attributes in __init__? . class Test: def __init__(self, a, b ,c): self.a, self.b, self.c = a, b, c . Ouch! That was painful. Look at all the repeated variable names. Do I really have to repeat myself like this when defining a class? Not Anymore! Checkout store_attr: . class Test: def __init__(self, a, b, c): store_attr() t = Test(5,4,3) assert t.b == 4 . You can also exclude certain attributes: . class Test: def __init__(self, a, b, c): store_attr(but=[&#39;c&#39;]) t = Test(5,4,3) assert t.b == 4 assert not hasattr(t, &#39;c&#39;) . There are many more ways of customizing and using store_attr than I highlighted here. Check out the docs for more detail. . . Avoiding subclassing boilerplate . One thing I hate about python is the __super__().__init__() boilerplate associated with subclassing. For example: . class ParentClass: def __init__(self): self.some_attr = &#39;hello&#39; class ChildClass(ParentClass): def __init__(self): super().__init__() cc = ChildClass() assert cc.some_attr == &#39;hello&#39; # only accessible b/c you used super . We can avoid this boilerplate by using the metaclass PrePostInitMeta. We define a new class called NewParent that is a wrapper around the ParentClass: . class NewParent(ParentClass, metaclass=PrePostInitMeta): def __pre_init__(self, *args, **kwargs): super().__init__() class ChildClass(NewParent): def __init__(self):pass sc = ChildClass() assert sc.some_attr == &#39;hello&#39; . . Type Dispatch . Type dispatch, or Multiple dispatch, allows you to change the way a function behaves based upon the input types it receives. This is a prominent feature in some programming languages like Julia. For example, this is a conceptual example of how multiple dispatch works in Julia, returning different values depending on the input types of x and y: . collide_with(x::Asteroid, y::Asteroid) = ... # deal with asteroid hitting asteroid collide_with(x::Asteroid, y::Spaceship) = ... # deal with asteroid hitting spaceship collide_with(x::Spaceship, y::Asteroid) = ... # deal with spaceship hitting asteroid collide_with(x::Spaceship, y::Spaceship) = ... # deal with spaceship hitting spaceship . Type dispatch can be especially useful in data science, where you might allow different input types (i.e. Numpy arrays and Pandas dataframes) to a function that processes data. Type dispatch allows you to have a common API for functions that do similar tasks. . Unfortunately, Python does not support this out-of-the box. Fortunately, there is the @typedispatch decorator to the rescue. This decorator relies upon type hints in order to route inputs the correct version of the function: . @typedispatch def f(x:str, y:str): return f&#39;{x}{y}&#39; @typedispatch def f(x:np.ndarray): return x.sum() @typedispatch def f(x:int, y:int): return x+y . Below is a demonstration of type dispatch at work for the function f: . f(&#39;Hello &#39;, &#39;World!&#39;) . &#39;Hello World!&#39; . f(2,3) . 5 . f(np.array([5,5,5,5])) . 20 . There are limitations of this feature, as well as other ways of using this functionality that you can read about here. In the process of learning about typed dispatch, I also found a python library called multipledispatch made by Mathhew Rocklin (the creator of Dask). . After using this feature, I am now motivated to learn languages like Julia to discover what other paradigms I might be missing. . . A better version of functools.partial . functools.partial is a great utility that creates functions from other functions that lets you set default values. Lets take this function for example that filters a list to only contain values &gt;= val: . test_input = [1,2,3,4,5,6] def f(arr, val): &quot;Filter a list to remove any values that are less than val.&quot; return [x for x in arr if x &gt;= val] f(test_input, 3) . [3, 4, 5, 6] . You can create a new function out of this function using partial that sets the default value to 5: . filter5 = partial(f, val=5) filter5(test_input) . [5, 6] . One problem with partial is that it removes the original docstring and replaces it with a generic docstring: . filter5.__doc__ . &#39;partial(func, *args, **keywords) - new function with partial application n of the given arguments and keywords. n&#39; . fastcore.utils.partialler fixes this, and makes sure the docstring is retained such that the new API is transparent: . filter5 = partialler(f, val=5) filter5.__doc__ . &#39;Filter a list to remove any values that are less than val.&#39; . . Composition of functions . A technique that is pervasive in functional programming languages is function composition, whereby you chain a bunch of functions together to achieve some kind of result. This is especially useful when applying various data transformations. Consider a toy example where I have three functions: (1) Removes elements of a list less than 5 (from the prior section) (2) adds 2 to each number (3) sums all the numbers: . def add(arr, val): return [x + val for x in arr] def arrsum(arr): return sum(arr) # See the previous section on partialler add2 = partialler(add, val=2) transform = compose(filter5, add2, arrsum) transform([1,2,3,4,5,6]) . 15 . But why is this useful? You might me thinking, I can accomplish the same thing with: . arrsum(add2(filter5([1,2,3,4,5,6]))) . You are not wrong! However, composition gives you a convenient interface in case you want to do something like the following: . def fit(x, transforms:list): &quot;fit a model after performing transformations&quot; x = compose(*transforms)(x) y = [np.mean(x)] * len(x) # its a dumb model. Don&#39;t judge me return y # filters out elements &lt; 5, adds 2, then predicts the mean fit(x=[1,2,3,4,5,6], transforms=[filter5, add2]) . [7.5, 7.5] . For more information about compose, read the docs. . . A more useful __repr__ . In python, __repr__ helps you get information about an object for logging and debugging. Below is what you get by default when you define a new class. (Note: we are using store_attr, which was discussed earlier). . class Test: def __init__(self, a, b=2, c=3): store_attr() # `store_attr` was discussed previously Test(1) . &lt;__main__.Test at 0x7fe0ab662790&gt; . We can use basic_repr to quickly give us a more sensible default: . class Test: def __init__(self, a, b=2, c=3): store_attr() __repr__ = basic_repr(&#39;a,b,c&#39;) Test(2) . Test(a=2, b=2, c=3) . . Monkey Patching With A Decorator . It can be convenient to monkey patch with a decorator, which is especially helpful when you want to patch an external library you are importing. We can use the decorator @patch from fastcore.foundation along with type hints like so: . class MyClass(int): pass @patch def func(self:MyClass, a): return self+a mc = MyClass(3) . Now, MyClass has an additional method named func: . mc.func(10) . 13 . Still not convinced? I&#39;ll show you another example of this kind of patching in the next section. . . A better pathlib.Path . When you see these extensions to pathlib.path you won&#39;t ever use vanilla pathlib again! A number of additional methods have been added to pathlib, such as: . Path.readlines: same as with open(&#39;somefile&#39;, &#39;r&#39;) as f: f.readlines() | Path.read: same as with open(&#39;somefile&#39;, &#39;r&#39;) as f: f.read() | Path.save: saves file as pickle | Path.load: loads pickle file | Path.ls: shows the contents of the path as a list. | etc. | . Read more about this here. Here is a demonstration of ls: . from pathlib import Path p = Path(&#39;../_notebooks&#39;) p.ls() # you don&#39;t get this with vanilla Pathlib.Path!! . (#21) [Path(&#39;../_notebooks/gpt2_simple_mask.jpg&#39;),Path(&#39;../_notebooks/bert_mac_small.jpg&#39;),Path(&#39;../_notebooks/causal_with_prefix.jpg&#39;),Path(&#39;../_notebooks/.DS_Store&#39;),Path(&#39;../_notebooks/2020-03-07-How_to_Create_an_Automatic_Code_Comment_Generator_using_Deep_Learning.ipynb&#39;),Path(&#39;../_notebooks/2020-09-01-fastcore.ipynb&#39;),Path(&#39;../_notebooks/2020-03-07-Syntax-Highlighting.ipynb&#39;),Path(&#39;../_notebooks/2020-03-06-bart.ipynb&#39;),Path(&#39;../_notebooks/README.md&#39;),Path(&#39;../_notebooks/2020-05-01-TrainDonkeyCar.ipynb&#39;)...] . Wait! What&#39;s going on here? We just imported pathlib.Path - why are we getting this new functionality? Thats because we imported the fastcore.foundation module, which patches this module via the @patch decorator discussed earlier. Just to drive the point home on why the @patch decorator is useful, I&#39;ll go ahead and add another method to Path right now: . @patch def fun(self:Path): return &quot;This is fun!&quot; p.fun() . &#39;This is fun!&#39; . That is magical, right? I know! That&#39;s why I&#39;m writing about it! . . An Even More Concise Way To Create Lambdas . Self, with an uppercase S, is an even more concise way to create lambdas that are calling methods on an object. For example, let&#39;s create a lambda for taking the sum of a Numpy array: . arr=np.array([5,4,3,2,1]) f = lambda a: a.sum() assert f(arr) == 15 . You can use Self in the same way: . f = Self.sum() assert f(arr) == 15 . Let&#39;s create a lambda that does a groupby and max of a Pandas dataframe: . import pandas as pd df=pd.DataFrame({&#39;Some Column&#39;: [&#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, ], &#39;Another Column&#39;: [5, 7, 50, 70]}) f = Self.groupby(&#39;Some Column&#39;).mean() f(df) . Another Column . Some Column . a 6 | . b 60 | . Read more about Self in the docs). . . Notebook Functions . These are simple but handy, and allow you to know whether or not code is executing in a Jupyter Notebook, Colab, or an Ipython Shell: . in_notebook(), in_colab(), in_ipython() . (True, False, True) . This is useful if you are displaying certain types of visualizations, progress bars or animations in your code that you may want to modify or toggle depending on the environment. . . A Drop-In Replacement For List . You might be pretty happy with Python&#39;s list. This is one of those situations that you don&#39;t know you needed a better list until someone showed one to you. Enter L, a list like object with many extra goodies. . The best way I can describe L is to pretend that list and numpy had a pretty baby: . define a list (check out the nice __repr__ that shows the length of the list!) . L(1,2,3) . (#3) [1,2,3] . Shuffle a list: . p = L.range(20).shuffle() p . (#20) [2,0,18,6,15,17,14,8,12,1...] . Index into a list: . p[2,4,6] . (#3) [18,15,14] . L has sensible defaults, for example appending an element to a list: . 1 + L(2,3,4) . (#4) [1,2,3,4] . There is much more L has to offer. Read the docs to learn more. . But Wait ... There&#39;s More! . There are more things I would like to show you about fastcore, but there is no way they would reasonably fit into a blog post. Here is a list of some of my favorite things that I didn&#39;t demo in this blog post: . Utilities . The Utilites section contain many shortcuts to perform common tasks or provide an additional interface to what standard python provides. . mk_class: quickly add a bunch of attributes to a class | wrap_class: add new methods to a class with a simple decorator | groupby: similar to Scala&#39;s groupby | merge: merge dicts | fasttuple: a tuple on steroids | Infinite Lists: useful for padding and testing | chunked: for batching and organizing stuff | . Multiprocessing . The Multiprocessing section extends python&#39;s multiprocessing library by offering features like: . progress bars | ability to pause to mitigate race conditions with external services | processing things in batches on each worker, ex: if you have a vectorized operation to perform in chunks | . Functional Programming . The functional programming section is my favorite part of this library. . maps: a map that also composes functions | mapped: A more robust map | using_attr: compose a function that operates on an attribute | . Transforms . Transforms is a collection of utilities for creating data transformations and associated pipelines. These transformation utilities build upon many of the building blocks discussed in this blog post. . Further Reading . It should be noted that you should read the main page of the docs first, followed by the section on tests to fully understand the documentation. . The fastcore documentation site. | The fastcore GitHub repo. | Blog post on delegation. | . Shameless plug: fastpages . This blog post was written entirely in a Jupyter Notebook, which GitHub automatically converted into to a blog post! Sound interesting? Check out fastpages. .",
            "url": "https://machinelearninghelsinki.github.io/mlstudygroup/fastcore/",
            "relUrl": "/fastcore/",
            "date": " • Sep 1, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://machinelearninghelsinki.github.io/mlstudygroup/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Youtube Playlist . Footnotes . This is the footnote. &#8617; . |",
            "url": "https://machinelearninghelsinki.github.io/mlstudygroup/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Us",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://machinelearninghelsinki.github.io/mlstudygroup/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://machinelearninghelsinki.github.io/mlstudygroup/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}